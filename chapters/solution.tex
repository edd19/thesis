\part{Solution} \label{part:solution}

\chapter{Implementation}

This chapter will be the occasion to detail the architecture of our solution. We will explain the technologies employed to achieve our goal, a monitoring software for Internet of Things networks. It is also in this chapter that we will justify the choices we made during the development.

\section{Architecture}

The architecture of our solution is composed of two parts as can be seen on Fig.\ref{fig:design}. The first part is about how the data is collected and sent. The second part consists of how the data is received and treated after reception. In the remaining of this section, strong assumptions are made that the reader is familiar with Netflow, particularly IPFIX, also referred as Netflow v10, and also its implementation for WSN which is called TinyIPFIX. If not, explanations about those particular technologies can be found in Chap \ref{chap:monitoring_tools}. \\

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{res/design.png}
	\caption{Global architecture of the solution}
	\label{fig:design}
\end{figure}

The data exportation is done in a Netflow fashion. The nodes occupy the role of exporters. Each node maintains a flow table where they register the flows that occurred during an interval of time. In our case, a flow is characterized by the 4-uple: source node id, destination node id, number packets and number octets. After a certain amount of time, each node will construct a TinyIPFIX message based on the content of its flow table. Those messages are sent to the gateway node. The gateway node will then reformat the TinyIPFIX messages received into compliant IPFIX messages to a specific address, which will be in our case the address of a server. The messages are sent using UDP. It is not reliable but using TCP to transmit data would incur the cost of initiating the connection and closing it. Also in a reliable network with a high packet loss percentage, resending information would increase the load on the nodes.\\

The server that receives the IPFIX messages from the IoT network plays two roles. One role is straightforward, it is the the role of collector. Each message is collected by the server and logged in a database for further processing and analyze. The other role is of a web server. Multiple users can know the network status by having access to web pages served by the server.

\section{Contiki modifications}

This section will tell about the modifications that we operated on the network stack of Contiki OS. Also we will explain the different codes used on our nodes as to effectively send the metadata about the networks.

\subsection{Network stack modification}

To effectively permit nodes to store the flows passing through them, it was inevitable to modify the Contiki network stack. First and foremost, we had to add an module who would be in charge of the maintenance of the flow table and, when a timeout occured or when the flow table attain its full capacity, send the corresponding TinyIPFIX messages. This also lead to the development of a library that would permit the construction of TinyIPFIX messages but also their conversion into IPFIX compliant messages (for the gateway node). The modified Contiki OS can be found on our Github repository \href{https://github.com/edd19/netflow_contiki}{netflow\_contiki}. The flow table code can be found in the \textit{/core/net/ipv6/ipv6flow} directory.\\

As stated before, we had to modify the network stack of Contiki OS. The reason is that in Contiki OS there is no event launched to notify that a node sent a message. This is quite cumbersome as we decided that the flow table of a node would only reflect the quantity of messages sent by that particular node. This means that each node is responsible to record the flows originated from it and send them. As to solve that issue we had to modify each function that would launch a message to update the flow table by describing to whom is the message addressed to and the size of the message. Our flow table is thus composed of the destination IPv6 address, the number of bytes sent and the number of packets. We do not need to store the source IPv6 address as it corresponds to the one of the node storing the flow table and is easily accessible. The Tab.\ref{table:flow_table} shows the different attributes composing the flow table. A flow occupy a total of 20 bytes.\\

\begin{table}
  \centering
  \begin{tabular}{|p{4.5cm}|p{3cm}|p{3.5cm}|}
    \hline
    Destination IPv6 address \newline (16 bytes) & Number of bytes \newline (2 bytes) & Number of packets \newline (2 bytes)\\
    \hline
  \end{tabular}
  \caption{Flow table}
  \label{table:flow_table}
\end{table}

Finally, the module in charge of the flow table permits to launch a process that would periodically send TinyIPFIX messages when a timeout occured (which we set to 5 minutes) or when the flow table reached its maximum capacity. More details in the next section.

\subsection{Sending of data}

This section will answer the questions relative on how the motes send TinyIPFIX messages, what triggers the sending of those meta information and what particular information is sent. Distinction must be done on the three types of motes: exporter, aggregator and gateway.\\

\begin{description}
  \item[Exporter] It is straightforward. It first sends the templates then sets a timer. When the timer has expired or when the flows table has reached its maximum capacity, the exporter exports the records. All packets, templates or data, are sent either directly to the gateway or to an aggregator node depending on the configuration. A bigger timer is used to send the records reflecting the status of the node (parent and battery). The Alg.\ref{algo:exporter} is the algorithm corresponding to an exporter.

  \begin{algorithm}
    \textbf{Function} Exporter:\\
    send templates\;
		send node status (TinyIPFIX)\;
    set small timer\;
		set big timer\;
    \While{true}{
      yield process until event\;
      \If{small timer timeout or flows table is full}{
        send flows (TinyIPFIX)\;
        empty flows table\;
        reset timer\;
      }

			\If{big timer timeout or modification of RPL parent}{
				send node status (TinyIPFIX)\
			}
    }
   \caption{Exporter}
   \label{algo:exporter}
  \end{algorithm}

  \item[Aggregator] It is an extension of the Exporter. It both merges and sends TinyIPFIX messages. At first, it sends the templates used then set a timer as well. When the aggregator has received an TinyIPFIX message, it merges that one with previous ones if any. When a timeout of the timer has occured or when the flows table of the aggregator has reached its limit, it will merge its records with the ones received then will send the new formed message to the gateway. This behavior is shown by the Alg.\ref{algo:aggregator}.

  \begin{algorithm}
    \textbf{Function} Aggregator:\\
    send templates\;
		send node status (TinyIPFIX)\;
    set small timer\;
		set big timer\;
    \While{true}{
      yield process until event\;
      \If{received TinyIPFIX message}{
        merge message with previous ones\;
      }
      \If{small timer timeout or flows table is full}{
        merge aggregator data/records (flows) with received ones\;
        send merged messages\;
        empty flows table\;
        reset timer\;
      }

			\If{big timer timeout or modification of RPL parent}{
			merge aggregator data/records (node status) with received ones\;
			send merged messages\;
			}
    }
   \caption{Aggregator}
   \label{algo:aggregator}
  \end{algorithm}

  \item[Gateway] It converts TinyIPFIX messages received into IPFIX messages. The converted messages are sent directly to the collector. This is shown by Alg.\ref{algo:gateway}.

  \begin{algorithm}
    \textbf{Function} Gateway:\\
    \While{true}{
      yield process until event\;
      \If{received TinyIPFIX message}{
        convert into IPFIX message;
        send converted message;
      }
    }
   \caption{Gateway}
   \label{algo:gateway}
  \end{algorithm}

\end{description}

The templates used are represented in Tab.\ref{table:traffic_template} and Tab.\ref{table:node_template}. The first one was used to send traffic information about the network. This template considers records of size of 8 bytes. Each record contains the source node id, the destination node id which is the receiving end of a flow and also the number of octets for that particular flows, plus the number of packets. All the fields mentioned occupy 2 bytes which leaves the possibility of having values up to 65535, which appears to be sufficient for the majority of WSNs. Indeed, apart for discovering the neighbors or organizing the routing topology, WSNs rarely communicate between themselves for the majority of usages. Thus, having 2 bytes for the number of octets appears to be a judicious choice. However, with IPFIX, the size of fields can be defined with the template and thus adapted to the network. The fields source node id and destination node id were defined for the purpose of this thesis. They are not standard ones, meaning not IANA defined. They are enterprise fields defined for this thesis. \\

The second template (Tab.\ref{table:node_template}) was used to send status information about the nodes. It gives information about the RPL parent of the node but also its battery left. For that template, each node should submit only one record. Also, records using that template should be submitted using a larger timeout than the previous ones or when a change in parent appear. We used those records to construct the RPL topology.

\begin{table}
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Source Node Id & 2 bytes \\
    \hline
    Destination Node Id & 2 bytes \\
    \hline
    Octets delta count & 2 bytes \\
    \hline
    Packets delta count & 2 bytes \\
    \hline
  \end{tabular}
  \caption{Traffic template}
  \label{table:traffic_template}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Source Node Id & 2 bytes \\
    \hline
    Parent Node Id & 2 bytes \\
    \hline
    Battery & 1 byte \\
    \hline
  \end{tabular}
  \caption{Node information template}
  \label{table:node_template}
\end{table}


\section{Technologies}

The monitoring tool developed for this thesis is a \textit{web-based} solution. We have chosen to write the monitoring tool with \textbf{Node.js} \cite{website:nodejs} which is a server-side solution for Javascript. In that sense, it offers modules or libraries to effectively develop HTTP server, i.e. a web server. To speed up our development, we used the \textit{Express framework} \cite{website:express} on top of Node.js. Express simplifies the routing of requests to views. A view can be seen as an web page. \\

The reasons we have chosen to write our monitoring tool as a web-server is the portability of such a solution. All OSs dispose of a web browser. For the user, it simplifies the utilization of the tool by simply accessing the web site via its preferred browser. The user does not have the trouble to download and install our software as with common desktop applications. Also, the user can freely access the tool from different locations or different computers. He is not limited to devices where the software is installed on. \\

Another factor in favor of a web solution is the ease with which multiple users can access the software at the same time. Most of the monitoring tools presented are desktop applications. They take the role of collectors and store the logs in a local database. This implies that the nodes transfer their data directly to the device having the monitoring tool on. However, a considerable drawback of such design is when multiple administrators exist for a same IoT network. Multiple users are bound to one device to monitor the network. One approach would be to retransmit the data sent by the nodes to each devices containing the monitoring tool in question, but it does not scale well. The web solution does not present that limitation. By having a centralized server playing the role of collector but also of web server, different users can efficiently monitor the network on their own device by simply requesting the web server. \\

One of the reasons for developing the software with Node.js and not other particular frameworks like Django or Ruby on Rails is due to the features that it offers via notably the Javascript language. First, it is event-driven and asynchronous by nature which is what we desire for our product. One such event to react to is when the collector receives an IPFIX message. Other events are for instance the status of a node changing as you will see later on. Programming by events proved to be powerful in our case. Secondly, the huge support on the Javascript language and its popularity which makes it a good choice for the adoption of the tool. Additionally, the huge documentations and modules helped us develop the tool faster and better. Thirdly but nonetheless, Javascript is tightly coupled to \acrfull{json} \cite{website:json} which is a data-interchange format. Objects in Javascript can be easily formated to JSON formated and exchanged via Internet. This allow to make dynamic website. Finally, we limit the language used during the process of production of the software mainly to Javascript for backend and HTML, CSS and Javascript for frontend where other frameworks would impose to master one or two more languages.\\

HTML, CSS and Javascript which are the core means to develop web pages proved to be useful but also easy to produce ways of visualizing the data. Javascript presents a lot of libraries to display graphs, charts and other type of figures. One such popular library is D3.js \cite{website:d3}. It also enables a website to be dynamic. Popular libraries like Bootstrap\cite{website:bootstrap} helped in making web page quicker and saved us the trouble to modify CSS files.\\

For the database, \textbf{PostgreSQL} \cite{website:postgresql} was used. It is an open-source relational database system. With more than 15 years of development, it does not have to prove anything anymore in term of reliability. It is SQL compliant. It recently added the support of JSON as field which enabled us to save IPFIX messages as JSON objects and not simply binary data.

\section{The monitoring tool}

This section will detail the different modules that compose our application plus the features that are offered by our application.

\subsection{Modules}

The application is comprised of four different modules :
\begin{itemize}
	\item Collector
	\item Log
	\item Nodes Status
	\item Web \\
\end{itemize}

The \textit{Collector} consists of a UDP server that listens to incoming IPFIX messages. Upon reception of a message, the Collector parses the binary data into an IPFIX object representing the original message. The object created is what will be used for the application to work with. It offers convenient methods to read records contained in the original IPFIX object. The Collector emits an event each time it creates an object from a received message. Modules can then listen and react to this particular event and treat the newly created object. \\

The \textit{Log} module offers logging capabilities of an IPFIX message. It also gives the possibility to query it. The storing is done in a PostgreSQL database. However, users can freely switch to its preferred SQL database as long as it also offers the possibilty to create a JSON field. To log an IPFIX message, it listens to the Collector's event mentioned before. It then stores in the database the JSON representation of the object. The module proposes methods to query the database such as having the different flows between two nodes as example. \\

To keep track of the status of the nodes, we deemed necessary to create a \textit{Nodes Status} module. This particular module also listens to the Collector's event for incoming IPFIX objects. It will then update the status of the nodes according to the content of the object. The status of a node comprises its parent, its battery, its number of bytes sent, number of packets sent and the last message sent, to name a few. The Nodes Status module offers the current state of the different nodes contained in the IoT network. To retrieve past values, it is necessary to pass via the Log module. The value of a status reflect the latest value received for that particular status. This means that if an IPFIX message contains the newest parent of the node, only the parent value of the node will be updated. The other status will remain the same.  \\

The final module is the \textit{Web} one. This module is essentially an HTTP server. Users interact with our tool mainly via this module. It permits users to visualize the status of their network via their browser by launching HTTP requests to which the server will reply by delivering the requested content. This module is an intermediate between the users and the Nodes Status module but also between the users and Log module. The reason for that is, to deliver content to the users, this module needs to gather data which is contained by the two aforementioned modules. This module can be seen as the frontend of our application whereas the other can be viewed as composing the backend of our solution.\\

The reasons behind using multiple modules is to achieve one of the requirements we imposed on ourselves, the \textit{modularity} of our program. We stated that a user should easily extend our software to add new features. By separating our applications into four different modules, we achieved this requirement. Each module offers different functionalities and abstractions. The Collector is responsible for collecting the IPFIX messages, the Log module to log those messages and provide means to query the logs, the Nodes Status keeps track of the status of the IoT network and finally the Web module permits users to interact and visualize the data. Users wanting to add new web pages will mostly only have to look into the Web module by adding another route to which the server will respond by replying by an HTML page. \\

The Fig.\ref{fig:modules} depicts the different modules presented and their interactions. As shown in the figure, the modules occupy different layers. The first layer is occupied by the Collector. The second one is occupied by the Nodes Status and Log modules. And finally, the third one is solely composed of the Web module. Those layers represent the level of interaction with the user with the top having the less interactions.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{res/modules.png}
	\caption{Modules interactions}
	\label{fig:modules}
\end{figure}

\subsection{Features}

The features offered by our monitoring applications are what we considered to be the basic ones. More description will be provided in the next chapter.

\begin{itemize}
	\item \textbf{Topology Map}: displays the logical topology of the WSN. The topology represents the RPL routing, meaning the topology is actually acyclic and represents a tree. A link represents a parent-child relation between two nodes. The root is the gateway node, and is colored differently to distinguish it from leaf nodes, which do not have children. Each node composing the topology is associated with its unique respective ID. Moreover, those nodes are uniquely clickable, one at a time, to get further information about their node values. Nodes are divided into three colors, i.e green, yellow and red, described their battery level.
	\item \textbf{Node values}: offers the possibility to see more in depth values sent by a particular node. It also gives meaningful statistics as the number of bytes sent by that node or the number of bytes that the nodes routed.
	\item \textbf{Network statistics}: gives statistics which are general to the whole network. This comprises the amount of traffic generated by the network. But also gives meaningful insight about the node sending the most data. This gives a broad view to the administrator.
\end{itemize}

\chapter{The Monitoring Tool - A web interface}

In this chapter, we will showcase and explain the features of our Monitoring software. As stated earlier, we have built a web interface for the monitoring of IoT networks. There are two main tabs, one that showcases \textit{traffic volume} evolving through time, and the other one that shows the \textit{topology} of the IoT network under visualization.

\section{Traffic Volume}

The Traffic Volume page shows how the traffic has evolved through time for a particular network. As can be seen on Fig.\ref{fig:tool_traffic}, this page is divided into six different panels. \\

The first panel contains a graph that shows the evolution of the traffic in term of bytes over time. More precisely, we consider intervals of five minutes. This graph contains two lines. The blue one is for normal traffic and the other one in orange is for IPFIX traffic. Normal traffic is all the traffic that is not Ipfix nor TinyIPFIX. With that, administrators can see if the bytes generated by the monitoring with TinyIPFIX is of great importance or not. \\

The second panel, top right, is a brief summary of the network. It shows how much bytes and packets were sent. But it also shows the average packet size. Those number are computed for three type of traffics: IPFIX/TinyIPFIX traffic, normal traffic and also normal plus IPFIX/TinyIPFIX traffic.\\

The third panel, bottom left, shows statistics about the IoT network. Those statistics are the minimum and maximum bytes generated for a particular interval. It also gives information about the average number of bytes sent by interval. Those statistics are only computed on normal traffic. Deduction is thus done on the IPFIX/TinyIPFIX traffic.\\

The fourth panel is a pie chart showing the distribution of the total traffic. It shows the distribution of the traffic between IPFIX messages, broadcast messages and unicast messages. Broadcast messages are often used in networking for networks to organize  themselves such as finding neighbors, assigning addresses or routing purposes. By showing the proportion of broadcast messages, administrators can have an idea about the amount of data used to form the RPL topology.\\

Finally, the fifth and sixth panels show the top five sources and top five destinations respectively. The top sources are the nodes that sent the most data while the top destinations are the nodes that received the most bytes. It permits to quickly see if for example a node presents a too large number of bytes sent compared to other nodes, or if a node is contacted too often.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{res/traffic.png}
	\caption{Traffic Volume}
	\label{fig:tool_traffic}
\end{figure}

\section{Current Topology}

\begin{figure}[!h]
	\centering
	\includegraphics[width=1.1\textwidth]{res/topology_interface.png}
	\caption{Topology Tab}
	\label{fig:topo}
\end{figure}

In this tab lies the topology of the network that is under visualization. Once all the traffic data has been received and processed by the server, it becomes possible to see the topology of our IoT network. As you can see in figure \ref{fig:topo}, the \textit{Motes topology} zone is where the graph is shown. \\

The traffic data used to build the topology is the last load of information received by the server from the IoT network. The gateway node forwards the IPFIX information collected by the nodes inside the network to the server which process it and update the routing topology. This is what mainly separate the Traffic Volume page from the Topology page. The Topology page is based only on the newest data while the other one use the logs stored in database. This page used only data provided by the \textit{Node Status} module explained in the chapter before.\\

It is worth noting that the topology is dynamic. Indeed, internally, it reconstructs itself every time new information is received from the gateway node. However a refresh of the page is needed to see the modified topology. Hence, topology changes on the web interface are not made punctually. \\

Evidently, each node is represented as a circle, the \textit{gateway mote} being in sky blue. Each grey link represents a connection "parent to child" in the IoT network. The bigger the circle used to represent the node, the bigger the nodes send data or route data. This is clearly seen in the Fig.\ref{fig:topo} where the node 4 is clearly bigger than node 3. Also, we also scale the link depending on the amount of traffic that pass through it. Bigger meaning a higher amount of data. It is possible to mouse over those links to see the amount of bytes that passed through it.\\

The Fig.\ref{fig:topo_info} shows information about the current state of the topology based on the freshest data. It shows the number of nodes composing the network. It also shows the maximum depth of the network. The depth of a node corresponds to the notion of depth for a tree. It is the number of edges from the node to the tree's route node (in our case the gateway node). Also, it gives information about the minimum battery left and the total volume of bytes and packets sent by the nodes.\\

All nodes in the topology are clickable, one at a time. It is not possible to click on a node if another node is already selected. Once a node is clicked on, its circle becomes larger and is filled in blue, as you can see in figure \ref{fig:snodeblue}. The \textit{Node status} panel (Fig.\ref{fig:snode}) allows to display specific information about the node we are interested in, namely its ID, its parent, its current battery level, and the total volume of data it has sent. It also gives information about the amount of data the node routed. The panel will use information based on the most recent messages received. There is also the \textit{Node flows} panel (Fig.\ref{fig:snode}) who shows the recent flows having as source the node selected.\\

As for the three different colors for regular nodes, we have decided to use them as indicators for battery levels, a node in green has more than 50\% of battery, a node in yellow lies between 50\% and 20\%, and a node in red is below 20\%. The colors could be used for other indicators in the network and with more tresholds, such as the total volume sent, or last time a node has sent information.\\

\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{res/snodeblue.png}
	\caption{Node clicked on}
	\label{fig:snodeblue}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{res/topology_info.png}
	\caption{Panel showing information about the network}
	\label{fig:topo_info}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{res/node_info.png}
	\caption{The panels showing specific data for a node}
	\label{fig:snode}
\end{figure}



\subsection*{The Dragging feature}

\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{res/populated.png}
	\caption{Populated topology, with out-of-border nodes}
	\label{fig:populated}
\end{figure}


We have decided to implement a dragging feature. That is, by clicking on a node and holding the click, it is possible to move the topology with the direction of the user's mouse. While this feature may seem insignificant, it finds its use when the topology gets populated. When there are more than 30 nodes (figure \ref{fig:populated}), the topology may expand further than the borders of the zone dedicated to the topology, as seen in figure \ref{fig:outnode}.\\



\begin{figure}
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{res/outnode.png}
        \caption{Out of borders nodes}
        \label{fig:outnode}
    \end{subfigure}
    ~
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{res/visible.png}
        \caption{Visible with dragging}
        \label{fig:visible}
    \end{subfigure}
    \caption{Dragging result}\label{fig:dragging}
\end{figure}

As you can see in figure \ref{fig:visible}, it is thus possible to fully see any group of nodes, since the topology zone is not unlimited in size. \\
